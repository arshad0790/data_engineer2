{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.apache.hadoop:hadoop-client-runtime:3.3.4\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bronze():\n",
    "    def __init__(self) -> None:\n",
    "        self.base_dir = \"Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/Kafka_Practice/data\"\n",
    "        self.BOOTSTRAP_SERVER = \"pkc-619z3.us-east1.gcp.confluent.cloud:9092\"\n",
    "        self.JAAS_MODULE = \"org.apache.kafka.common.security.plain.PlainLoginModule\"\n",
    "        self.CLUSTER_API_KEY = \"WAMFMTSHUDP2D3I3\"\n",
    "        self.CLUSTER_API_SECRET = \"VXnP+yvvBC1F5nmO3J5pYEJ3GEwQj/YFE8GLDdmi9WloeaDhjo4AvW9Eoi6AEg2Y\"\n",
    "        self.invoice_schema = \"\"\"InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "                        CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint, \n",
    "                        PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, \n",
    "                        DeliveryType string,\n",
    "                        DeliveryAddress struct<AddressLine string, City string, ContactNumber string, PinCode string, \n",
    "                        State string>,\n",
    "                        InvoiceLineItems array<struct<ItemCode string, ItemDescription string, \n",
    "                            ItemPrice double, ItemQty bigint, TotalValue double>>\n",
    "                        \"\"\"\n",
    "\n",
    "    def ingest_from_kafka(self, start_timestamp=1):\n",
    "        return (spark.readStream\n",
    "            .format(\"kafka\")\n",
    "            .option(\"kafka.bootstrap.servers\", self.BOOTSTRAP_SERVER)\n",
    "            .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "            .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "            .option(\"kafka.sasl.jaas.config\", f\"{self.JAAS_MODULE} required username='{self.CLUSTER_API_KEY}' password='{self.CLUSTER_API_SECRET}';\")\n",
    "            .option(\"subscribe\", \"invoices\")\n",
    "            .option(\"maxOffsetsPerTrigger\", 10)\n",
    "            .option(\"startingTimestamp\", start_timestamp)\n",
    "            .load()\n",
    "            )\n",
    "    \n",
    "    def get_invoices(self, kafka_df: DataFrame):\n",
    "        return (kafka_df.select(col('key').cast(\"string\").alias(\"key\"),\n",
    "                                from_json(col(\"value\").cast(\"string\"), self.invoice_schema).alias(\"value\"),\n",
    "                                \"topic\", \"timestamp\")\n",
    "                                )    \n",
    "    \n",
    "    def process(self, start_timestamp=1):\n",
    "        print(\"Starting Bronze Stream...\", end='')\n",
    "        kafka_df = self.ingest_from_kafka(start_timestamp)\n",
    "        invoice_df = self.get_invoices(kafka_df)\n",
    "        sQuery =  (invoice_df.writeStream\n",
    "                    .queryName(\"bronze_query\")\n",
    "                    .format(\"parquet\")\n",
    "                    .option(\"checkpointLocation\", f\"{self.base_dir}/chekpoint/invoices\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .start(\"/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/Kafka_Practice/data/output\"))\n",
    "        print(\"Done\")\n",
    "        return sQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bronze Stream...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 11:37:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f8462b960b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/12 11:37:22 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bronze_kafka = Bronze()\n",
    "bronze_kafka.process()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/Kafka_Practice/data/output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
