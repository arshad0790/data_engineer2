{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/01/28 16:11:24 WARN Utils: Your hostname, DESKTOP-8147UDP resolves to a loopback address: 127.0.1.1; using 172.18.36.72 instead (on interface eth0)\n",
      "25/01/28 16:11:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/28 16:11:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataFrame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39minvoiceStream\u001b[39;00m():\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_data_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/landing\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflattenInvoices\u001b[39m(\u001b[39mself\u001b[39m, explodedDF):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m( explodedDF\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mItemCode\u001b[39m\u001b[39m\"\u001b[39m, expr(\u001b[39m\"\u001b[39m\u001b[39mLineItem.ItemCode\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m                     \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mItemDescription\u001b[39m\u001b[39m\"\u001b[39m, expr(\u001b[39m\"\u001b[39m\u001b[39mLineItem.ItemDescription\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m                     \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mItemPrice\u001b[39m\u001b[39m\"\u001b[39m, expr(\u001b[39m\"\u001b[39m\u001b[39mLineItem.ItemPrice\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m                     \u001b[39m.\u001b[39mdrop(\u001b[39m\"\u001b[39m\u001b[39mLineItem\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m             )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappendInvoices\u001b[39m(\u001b[39mself\u001b[39m, flattenedDF: DataFrame):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (flattenedDF\u001b[39m.\u001b[39mwriteStream\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m                 \u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mparquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m                 \u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mcheckpointLocation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_data_dir\u001b[39m}\u001b[39;00m\u001b[39m/chekpoint/invoices\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m                 \u001b[39m.\u001b[39moutputMode(\u001b[39m\"\u001b[39m\u001b[39mappend\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m                 \u001b[39m.\u001b[39mstart(\u001b[39m\"\u001b[39m\u001b[39m/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/output/inv-stream\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/invoice_streaming.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataFrame' is not defined"
     ]
    }
   ],
   "source": [
    "class invoiceStream():\n",
    "    def __init__(self):\n",
    "        self.base_data_dir = \"/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/landing\"\n",
    "\n",
    "        self.json_schema = \"\"\"InvoiceNumber string, CreatedTime bigint, StoreID string, PosID string, CashierID string,\n",
    "                        CustomerType string, CustomerCardNo string, TotalAmount double, NumberOfItems bigint, \n",
    "                        PaymentMethod string, TaxableAmount double, CGST double, SGST double, CESS double, \n",
    "                        DeliveryType string,\n",
    "                        DeliveryAddress struct<AddressLine string, City string, ContactNumber string, PinCode string, \n",
    "                        State string>,\n",
    "                        InvoiceLineItems array<struct<ItemCode string, ItemDescription string, \n",
    "                            ItemPrice double, ItemQty bigint, TotalValue double>>\n",
    "                    \"\"\"\n",
    "    \n",
    "    def readInvoices(self):\n",
    "        return (spark.readStream\n",
    "                .format(\"json\")  \n",
    "                .schema(self.json_schema)   \n",
    "                .load(f\"{self.base_data_dir}/data/invoices\")) \n",
    "    \n",
    "    def explodeInvoices(self, invoiceDF):\n",
    "        return ( invoiceDF.selectExpr(\"InvoiceNumber\", \"CreatedTime\", \"StoreID\", \"PosID\",\n",
    "                                      \"CustomerType\", \"PaymentMethod\", \"DeliveryType\", \"DeliveryAddress.City\",\n",
    "                                      \"DeliveryAddress.State\",\"DeliveryAddress.PinCode\", \n",
    "                                      \"explode(InvoiceLineItems) as LineItem\")\n",
    "                                    )  \n",
    "\n",
    "    def flattenInvoices(self, explodedDF):\n",
    "        return( explodedDF.withColumn(\"ItemCode\", expr(\"LineItem.ItemCode\"))\n",
    "                        .withColumn(\"ItemDescription\", expr(\"LineItem.ItemDescription\"))\n",
    "                        .withColumn(\"ItemPrice\", expr(\"LineItem.ItemPrice\"))\n",
    "                        .withColumn(\"ItemQty\", expr(\"LineItem.ItemQty\"))\n",
    "                        .withColumn(\"TotalValue\", expr(\"LineItem.TotalValue\"))\n",
    "                        .drop(\"LineItem\")\n",
    "                )\n",
    "\n",
    "    def appendInvoices(self, flattenedDF: DataFrame):\n",
    "        return (flattenedDF.writeStream\n",
    "                    .format(\"parquet\")\n",
    "                    .option(\"checkpointLocation\", f\"{self.base_data_dir}/chekpoint/invoices\")\n",
    "                    .outputMode(\"append\")\n",
    "                    .start(\"/root/data_engineer2/Apache-Spark-and-Databricks-Stream-Processing-in-Lakehouse-main/my_practice/output/inv-stream\")\n",
    "        )\n",
    "    \n",
    "    def process(self):\n",
    "        print(f\"Starting Invoice Processing Stream...\", end='')\n",
    "        invoicesDF = self.readInvoices()\n",
    "        explodedDF = self.explodeInvoices(invoicesDF)\n",
    "        resultDF = self.flattenInvoices(explodedDF)\n",
    "        sQuery = self.appendInvoices(resultDF)\n",
    "        print(\"Done\\n\")\n",
    "        return sQuery     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Invoice Processing Stream...Done\n",
      "\n",
      "No physical plan. Waiting for data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/04 10:43:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/12/04 10:43:45 WARN StreamingQueryManager: Stopping existing streaming query [id=07c62567-0f82-4b53-a416-c9969382a5ce, runId=c415efb1-5289-4b2f-82f2-4e7d0cd9ddad], as a new run is being started.\n"
     ]
    }
   ],
   "source": [
    "inv_stream = invoiceStream()\n",
    "sQuery = inv_stream.process()\n",
    "sQuery.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sQuery.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
